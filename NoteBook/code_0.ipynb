{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import is_classifier\n",
    "class FeatureSelector:\n",
    "    def __init__(self, model, cv=5, scoring='accuracy'):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.rfecv = None\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.rfecv = RFECV(estimator=self.model, step=1, cv=self.cv, scoring=self.scoring)\n",
    "        self.rfecv.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        return X[:, self.rfecv.support_]\n",
    "\n",
    "    def get_selected_features(self, feature_names):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        return [feature_names[i] for i, is_selected in enumerate(self.rfecv.support_) if is_selected]\n",
    "\n",
    "    def get_feature_ranking(self, feature_names):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        ranking = self.rfecv.ranking_\n",
    "        return pd.Series(ranking, index=feature_names)\n",
    "\n",
    "    def get_selected_feature_indices(self):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        return np.where(self.rfecv.support_)[0]\n",
    "\n",
    "    def get_num_selected_features(self):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        return self.rfecv.n_features_\n",
    "        \n",
    "        \n",
    "    def remove_features(self, X, model_name, importance_threshold=0.01):\n",
    "        if self.rfecv is None:\n",
    "            raise ValueError(\"The model has not been fitted. Call fit_transform method first.\")\n",
    "\n",
    "        # Check if importance_threshold is valid\n",
    "        if not 0 <= importance_threshold <= 1:\n",
    "            raise ValueError(\"importance_threshold should be a value between 0 and 1.\")\n",
    "\n",
    "        print(f\"--------------------\")\n",
    "\n",
    "        print(f\"Model : {model_name}\")\n",
    "\n",
    "        # Get the names of features to be removed\n",
    "        feature_names = self.get_selected_features(X.columns)\n",
    "\n",
    "        # Get the feature importances\n",
    "        feature_importances = self.rfecv.estimator_.feature_importances_\n",
    "\n",
    "        # Determine the number of features to remove based on the threshold\n",
    "        num_features_to_remove = np.sum(feature_importances < importance_threshold)\n",
    "\n",
    "        # If no features need to be removed, return the original X\n",
    "        if num_features_to_remove == 0:\n",
    "            print(\"No features need to be removed.\")\n",
    "            return X\n",
    "\n",
    "        # Print the feature importances\n",
    "        print(\"Feature Importances:\")\n",
    "        for feature_name, importance in zip(feature_names, feature_importances):\n",
    "            print(f\"{feature_name}: {importance}\")\n",
    "\n",
    "        # Get the feature indices to be removed\n",
    "        feature_indices_to_remove = np.argsort(feature_importances)[:num_features_to_remove]\n",
    "\n",
    "        # Print the features to be removed\n",
    "        print(\"                 ----------------------------                       \")\n",
    "        print(\"Features to be removed:\")\n",
    "        for idx in feature_indices_to_remove:\n",
    "            print(f\"  - {feature_names[idx]}\")\n",
    "\n",
    "\n",
    "models=ModelClassifier()\n",
    "\n",
    "\n",
    "for model_name, model in models.model_dict.items():\n",
    "    \n",
    "    if model_name==best_model:\n",
    "        \n",
    "        # Create an instance of FeatureSelector for each model\n",
    "        feature_selector = FeatureSelector(cv=5, scoring='accuracy',model=model)\n",
    "        \n",
    "        # Perform feature selection with RFECV for each model\n",
    "        X=X_train.values\n",
    "        y=y_train.values\n",
    "        \n",
    "        X_train_selected = feature_selector.fit_transform(X, y)\n",
    "        \n",
    "        x_val=X_test.values\n",
    "        X_val_selected = feature_selector.transform(x_val)\n",
    "        \n",
    "        # Get the selected feature indices\n",
    "        selected_feature_indices = feature_selector.get_selected_feature_indices()\n",
    "        # Print the selected feature indices before removal\n",
    "        print(\"Selected feature indices before removal:\", selected_feature_indices)\n",
    "        \n",
    "        feature_selector.remove_features(X=X_train,model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna catboost without split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class OptunaTuner_Catboost:\n",
    "    def __init__(self, model, params, X, y):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def Objective(self, trial):\n",
    "        param_values = {}\n",
    "        for key, value_range in self.params.items():\n",
    "            if value_range[0] <= value_range[1]:\n",
    "                if isinstance(value_range[0], int) and isinstance(value_range[1], int):\n",
    "                    param_values[key] = trial.suggest_int(key, value_range[0], value_range[1])\n",
    "                else:\n",
    "                    param_values[key] = trial.suggest_float(key, value_range[0], value_range[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid range for {key}: low={value_range[0]}, high={value_range[1]}\")\n",
    "\n",
    "        model = CatBoostClassifier(**param_values)\n",
    "\n",
    "        # Evaluate the model using AUC-ROC\n",
    "        model.fit(self.X, self.y)  # Fit the model on the data\n",
    "        y_probs = model.predict_proba(self.X)[:, 1]  # Get predicted probabilities for the positive class\n",
    "        auc_roc = roc_auc_score(self.y, y_probs)\n",
    "        return auc_roc\n",
    "\n",
    "    def tune(self, n_trials=100):\n",
    "        study = optuna.create_study(direction=\"maximize\")  # maximize AUC-ROC\n",
    "        study.optimize(self.Objective, n_trials=n_trials)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        # Create a new CatBoost model instance with the best parameters\n",
    "        best_model = CatBoostClassifier(**best_params)\n",
    "        best_model.fit(self.X, self.y)  # Fit the best model on the data\n",
    "\n",
    "        best_auc_score = study.best_value\n",
    "        print(f\"Best AUC Score: {best_auc_score}\")\n",
    "\n",
    "        # Here, we return both the tuned model and the best AUC-ROC score\n",
    "        return best_auc_score, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna with Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class OptunaTuner:\n",
    "    def __init__(self, model, params, X, y):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def Objective(self, trial):\n",
    "        param_values = {}\n",
    "        for key, value_range in self.params.items():\n",
    "            if value_range[0] <= value_range[1]:\n",
    "                if isinstance(value_range[0], int) and isinstance(value_range[1], int):\n",
    "                    param_values[key] = trial.suggest_int(key, value_range[0], value_range[1])\n",
    "                else:\n",
    "                    param_values[key] = trial.suggest_float(key, value_range[0], value_range[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid range for {key}: low={value_range[0]}, high={value_range[1]}\")\n",
    "\n",
    "        self.model.set_params(**param_values)\n",
    "\n",
    "        # Evaluate the model using AUC-ROC\n",
    "        self.model.fit(self.X, self.y)  # Fit the model on the data\n",
    "        y_probs = self.model.predict_proba(self.X)[:, 1]  # Get predicted probabilities for the positive class\n",
    "        auc_roc = roc_auc_score(self.y, y_probs)\n",
    "        return auc_roc\n",
    "\n",
    "    def tune(self, n_trials=100):\n",
    "        study = optuna.create_study(direction=\"maximize\")  # maximize AUC-ROC\n",
    "        study.optimize(self.Objective, n_trials=n_trials)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        self.model.set_params(**best_params)\n",
    "\n",
    "        print(f\"Best AUC Score: {study.best_value}\")\n",
    "\n",
    "        # Here, we return both the tuned model and the best AUC-ROC score\n",
    "        return study.best_value, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
